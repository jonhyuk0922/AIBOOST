## 오늘의 회고 : 수학이 없으니 행복했다.. 내일은 토치와 더 친해지자.


- Intro] 인공지능의 정의 및 딥러닝 플로우
    1. 인공지능은 사람의 지능을 모방한 것이다. 포함관계는 인공지능 > 머신러닝 > 딥러닝으로 결국 딥러닝은 인공지능의 일부이다.
    - 딥러닝에서 중요한 요소들
- 1.Deep Learning's Most Important Ideas - A Brief Historical Review
    - 2020년 기준, 딥러닝에 있어 중요한 사건들의 흐름 정리

    2012(AlexNet, CNN) - 224 * 224 이미지를 분류하는 것 , DL이 처음 이미지넷 우승함

    2013(DQN) - 딥마인드가 아케이드 게임을 깨기 위해 만든 모델

    2014(Encoder - Decoder) - 인코더는 단어의 시퀀스를 벡터로 임베딩하고 임베딩된 벡터를 다시 다른 언어의 시퀀스로 만들어 주는 것 , 기계번역의 트렌드를 바꿈

    2014(Adam Optimizer) - 왜 아담을 사용할까? 결과가 잘나와서 그렇다. (논문에도 설명이없다) 

    2015(GAN, Generative Adversarial Network) 

    2015(Residual Networks) - 잔차학습 , 딥러닝이 딥러닝 답게 되었다. 이전엔 과대적합때문에 딥해질 수 없었다. 잔차학습을 통해 전보다 더 딥하게 쌓을 수 있게 만들어줌,

    2017(Transformer) - 이게 왜 RNN보다 좋은 지 설명

    2018(BERT, fine-tuned NLP models) - 큰 말뭉치를 학습한 모델에 파인튜닝을 해서 다운스트림 테스크를 해결함

    2019(BIG Language Models , GPT-3) - 약간의 파인튜닝을 통해 다양한 분야에 적용할 수 있다. 

    2020(Self Supervised Learning) - SimCLR : a simple framework for contrastive learning of visual representations , GNN도 이중 하나 인듯

- 2.Linear Neural Networks & How to learn
    - 선형 신경망(Linear Neural Networks) : 예측한 y와 y의 차이를 줄여가는 방향으로 학습한다.
    - 파라미터(w,b)가 어느 방향으로 움직였을 때 loss가 줄어드는 지 찾고(loss를 파라미터로 편미분), 그 방향으로 업데이트하는 것(최적화, 편미분한 값에 stepsize를 곱해서 기존파라미터에서 빼준다)이 목표이다.
    - stepsize(~ learning rate) 를 설정할 때, 미분은 순간변화율(local한 값)이므로 stepsize를 너무 크게하면 학습이 유효하지 않을 수 있다.
- 3.Neural Networks are function approximators that stack affine transformations followed by nonlinear transformations

    : 신경망은 아핀 변환에 이어 비선형 변환을 쌓는(stack) 함수 근사이다. 

    - affine transformations 란?
    - nonlinear transformations 가 필요한 이유 , ppt 12~13p를 보면 W 두개를 서로 곱해주는 건 결국 행렬간의 곱으로 한개의 히든 레이어로도 표현할 수 있게된다. 그러므로 비선형 변환이 필요하다. 그래야 신경망을 깊게 쌓을 수 있다.
- 4.Multilayer Feedforward Networks are Universal Approximators

    이것을 만족하는 싱글레이어 뉴럴넷이 세상 어딘가에 존재한다는 존재성만 증명 ,  어떻게 찾을 지는 언급하지 않는다. 

- 5. Multi-Layer-Perceptron
    - Task별 손실함수

    MSE가 너무 크면 전반적인 신경망이 망가진다. 즉, 도움이 안될 수도 있다.

    CE , 분류문제의 아웃풋은 대부분 원핫 벡터로 나온다. d개의 아웃풋이 나왔을 떄 나온 값중 가장 큰 값(100 이든 10이든) 만 나오면 분류를 잘할 수 있다. 

    → 과연 CE가 분류문제일 떄 최적일까?

    MLE , Probablisistic task

- 질문1 : 과연 CE가 분류문제 loss func으로 최적일까?

- 질문2 : relu 함수가 0일 때 미분 불가능한데 오차역전파법 적용할 때 어떻게 하는 지?

    relu 함수에서 0보다 작은 값은 미분값0으로 , 0보다 큰 부분은 미분값 1로 나타낼수 있다. 다만 x=0인 부분에선 미분값은 0 , 0.5, 1 중 한가지 값으로 하고 있다. 세가지중 뭘하든 별 상관없고 보통은 1을 한다.

- 질문3 : 파라미터 초기화는 왜 하며, 어떠한 방식으로 작동하여 효과가 있는건가?

    위의 질문에 대해 저도 간단히 추가적인 첨언을 드리자면 가중치 초기화(weight initialization)는 딥러닝 학습의 어려움을 극복하기 위한 방법중 하나입니다.최적화(optimization)에서 어떤 초기값에서 출발하느냐에 따라 local minimum 혹은 global minimum에 빠지느냐가 결정되기도 합니다. 아무리 좋은 optimizer를 사용하여도 초기값을 잘못 설정하면 global minimum으로 수렴하기가 상당히 어렵습니다. 그래서 초기값을 올바르게 설정하는 것은 매우 중요합니다. 하지만 딥러닝 모델은 거대한 feature space를 가지고 있기 때문에 올바른 초기값을 설정하는 것은 어렵습니다. 그래서 조금 다른 목적으로 사용됩니다.

    - forward시에는 전달되는 값이 너무 너무 작아지거나 커지지 않도록
    - backward시에는 gradient값이 너무 작아지거나(gradient vanishing) 너무 커지지 않도록(gradient exploding)

    가중치의 값을 적절히 초기화를 해야 합니다.간단히 이야기해 가중치 초기화를 하지 않게 되면 gradient값이 소실되거나 증폭되어 적절한 학습이 이뤄지지 않을 수 있습니다. 가중치 초기화는 정해진 규칙이 없으며 경험에 의존해서 초기화 값을 설정하게 됩니다.추가적으로 uniform, normal 분포로 가중치를 초기화하는 이유는 symmetric breaking을 위해서인데 이에 대해서는 아래 링크에 간단히 설명되어 있습니다.[https://stats.stackexchange.com/questions/45087/why-doesnt-backpropagation-work-when-you-initialize-the-weights-the-same-value](https://stats.stackexchange.com/questions/45087/why-doesnt-backpropagation-work-when-you-initialize-the-weights-the-same-value)

    TL;DR: 딥러닝 모델의 학습을 위해선 gradient가 back propagation시에 적절히 잘 이뤄져야하고 이는 가중치의 값에 결정되기 때문에, 적절한 값으로 가중치 초기화 해야합니다. 안해도 큰 문제는 없을 수 있는데 일반적으로 학습이 원활히 이뤄지지 않아 90% 성능이 50%로 하락할 수 있습니다.

    [머신러닝 - Weight Initialization(가중치 초기화) 중요성&문제점&종류](https://blog.naver.com/PostView.nhn?blogId=handuelly&logNo=221831940317&parentCategoryNo=&categoryNo=23&viewDate=&isShowPopularPosts=false&from=postView)

- 질문4 : 티스토리나 깃헙에 수식 마크다운 적용하는 방법은?
- 질문5 : 엔트로피와 크로스 엔트로피의 차이는?

    (정보량 and 엔트로피) [https://angeloyeo.github.io/2020/10/26/information_entropy.html](https://angeloyeo.github.io/2020/10/26/information_entropy.html)
    (크로스 엔트로피 and KL-Divergence) [https://angeloyeo.github.io/2020/10/27/KL_divergence.html](https://angeloyeo.github.io/2020/10/27/KL_divergence.html)

    KL-Divergence (두 확률분포의 차이) = P(x)의 정보 엔트로피 - P(x) 기준 Q(x) 의 크로스 엔트로피 (손실함수)
    => 크로스 엔트로피 최소화의 의미 = P(x) 와 Q(x)가 비슷해지는 정도
    => 당연히 P(x) = Q(x) 일 때, KL-Divergence 값이 0이 되므로 가장 좋은 성능이겠지만 사실상 거의 불가능

적용점 

1. 모두의 딥러닝_파이토치 DNN 듣고 오기
2. CNN , RNN 에 대해 자기만의 언어로 정리해보기
