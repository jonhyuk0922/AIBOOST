오늘의 회고 : 내 머리 RAM이 꽉찼다. 매일 정리하자.

> 강의 목표 : 최적화와 관련된 주요한 용어와 다양한 Gradient Descent 기법들을 배웁니다.

- 주요한 용어: **Generalization, Overfitting,  Cross-validation** 등 다양한 용어가 있습니다. 각 용어들의 의미에 대해 배웁니다.
- 다양한 Gradient Descent 기법: 기존 SGD(Stochastic gradient descent)를 넘어서 최적화(학습)가 더 잘될 수 있도록 하는 다양한 기법들에 대해 배웁니다.
- 마지막으로,  Gradient Descent 기법에 따른 성능의 변화를 알아보는 실습을 합니다.

- 0.Intro
- 1.최적화에서 주요한 용어들(Important Concepts in Optimization)
- 2.Practical Gradient Descent Methods

    보통 미니 배치를 가장 많이 사용한다. 미니 배치를 사용할 때 배치 사이즈를 작게하면 Flat minimum 즉 전반적으로 좋은 값이 나온다. 하지만 배치 사이즈를 크게하면 Sharp Minimum 즉 한개의 가장 좋은 값이 나오나 전반적으론 편차가 생긴다. (경험상 128 or 256 이 최고인듯)

    - 프레임워크에서 Optimizer를 제공한다. 다만 어떤걸 쓸 지는 골라야한다.
        1. (Stochastic) Gradient descent : 가중치에서 lr*gradient를 곱해서 빼주는 과정을 통해 업데이트한다. 이 과정에선 lr을 잡아주는 것이 중요하다.
        2. Momentum : 어떻게 하면 더 빨리 학습을 시킬 수 있을까에서 고안된 개념으로, 한번 흘러가기 시작한 그레디언트를 어느 정도 유지해서, 여러번 왔다갔다해도 잘 유지되도록 하는 것이 목표다.
        3. Neserov Accelerated Gradient : 더 빠르게 수렴한다. (모멘텀보다 지그재그를 안한다? 정도)
        4. Adagrad : 많이 변한 파라미터는 덜 변화시키고, 덜 변한 파라미터는 많이 변화시킨다. 이 변화의 정도를 나타내는 게 G이다.
        5. Adadelta : lr이 없다. 바꿀 수 있는 요소가 많이 없다.
        6. RMSprop : stepsize를 넣어줬다. ← 무슨 효과일까?
        7. Adam : 입실론을 잘 바꿔주는 것이 중요하다.
- 3.규제(Regularization)
    - 과대적합을 피하기 위한 방법 , 일반화가 잘되도록 하는 것
    - 다양한 방법들
        1. Early stopping : validation set을 통해 일정 기준이 되었을 때 빨리 멈추는 것
        2. Parameter norm penalty : 부드러운 함수일수록 일반화 잘될 것이라는 전제하에 웨이트를 함께 줄어주는 것. weight decay라고도 부른다. 
        3. Data augmentation : 데이터가 많을 수록 과대적합 피할 수 있다. (왜냐면 다양한 경우를 만날 수 있으니까.. e.g) 서있는 강아지 돌려서 누워있는 강아지 학습에 도움주는 데이터 증강 가능해짐)
        4. Noise Robustness : 입력데이터와 웨이트에 노이즈를 넣어준다. 이렇게 하면 성능이 더 잘 나온다. (왜 잘되는 지 모름)
        5. Label smoothing : 데이터 두개를 뽑아서 섞어준다. 이를 통해 이미지 분류같은 경우 결정기준을 부드럽게 만들어준다.  (Mix up , CutMix) 가성비가 좋다 ㅎ..
        6. Dropout : 주변 뉴런들은 서로 비슷한 가중치를 학습하기에 일정 비율 뉴런을 랜덤하게 꺼준다.
        7. Batch normalization : 정규화한다는 의미인듯.. 추가 공부 필요함.

참고자료 

Bias and Variance Tradeoff : [https://modulabs-biomedical.github.io/Bias_vs_Variance](https://modulabs-biomedical.github.io/Bias_vs_Variance)

Batch size in deep learning 논문 리뷰 : [https://blog.lunit.io/2018/08/03/batch-size-in-deep-learning/](https://blog.lunit.io/2018/08/03/batch-size-in-deep-learning/)

Optimizer : [https://dev-jm.tistory.com/10](https://dev-jm.tistory.com/10)

앙상블 학습 : [https://m.blog.naver.com/qbxlvnf11/221488622777](https://m.blog.naver.com/qbxlvnf11/221488622777)

GD optimizer : [http://shuuki4.github.io/deep learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)

오토인코더 : [https://excelsior-cjh.tistory.com/m/187](https://excelsior-cjh.tistory.com/m/187)

코드와 함께 설명된 모델들 : [http://einops.rocks/pytorch-examples.html](http://einops.rocks/pytorch-examples.html)

트랜스포머 : [https://hoya012.github.io/blog/Vision-Transformer-1/](https://hoya012.github.io/blog/Vision-Transformer-1/)

weight init : [https://deepinsight.tistory.com/114](https://deepinsight.tistory.com/114)

[https://nittaku.tistory.com/269?category=742607](https://nittaku.tistory.com/269?category=742607)

batch normalization : [https://eehoeskrap.tistory.com/430](https://eehoeskrap.tistory.com/430)

inductive bias : [https://velog.io/@euisuk-chung/Inductive-Bias란](https://velog.io/@euisuk-chung/Inductive-Bias%EB%9E%80)
