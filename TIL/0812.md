# 1.오늘 배운 내용 & 피어세션

## 1. Sequential model - RNN, LSTM , GRU

> sequential model은 무엇이고, 다루는데 어려움은 무엇일까?

- Sequential model : Naive sequence model, autoregressive model, markov model(first-order autoregressive model) , latent autoregressive model
- sequential model의 목표는 sequential data의 레이블을 얻는 것인데, 길다보니 그 값을 구하기 쉽지 않다. 그래서 일부만 보는 방법이 고안되었다.

> RNN : Recurrent Neural Network

RNN 의 부족한 점 : Short-term dependencies 즉, 멀리있는 걸 고려하기 힘들다. 

즉, relu를 사용할 경우, weight가 폭발하고 sigmoid를 사용할 경우 weight가 사라지는 문제가 있다.

> LSTM : Long Short Term Memory

LSTM core idea :  활성화함수로 tan(h) 사용,  중간에 흘러가는 cell state(컨베이너 벨트같은 역할)을 만들어줘서 long term memory가 가능하도록 만들어준다. 

> GRU

 gate가 두개뿐으로 LSTM(gate 3개) 보다 파라미터 수가 줄어든다. LSTM과 같은 성능을 낼 때 파라미터가 더 적은 GRU가 더 단순한 모델로, 일반화 성능이 더 뛰어나다.

## 2. Sequential model - Transformer

수업시간엔 인코더부분이랑 Multi head Attention만 다룰 것이다.

입력 seq 의 길이와 출력 seq은 다를 수 있구나, 입력과 출력 도메인이 다를 수 있구나.

- 인코더가 n개의 단어를 어떻게 한번에 처리할 수 있을까?

> Multi-Head Attention

- Encoder : input embedding이 256 dim이고 8개의 head가 존재한다면,  각 단어의 input embedding 을 32 차원씩 나눠서 각각의 head로 넣어준다.

    즉, 모든 head는 모든 단어들을 학습하지만, 각 단어의 다른 면을 학습하므로 좋은 성능을 낼 수 있다.

- Decoder : 뒤에 나오는 단어들을 마스킹해줘서 치팅 효과를 방지해준다.

> VIT : Vision Transformer

VIT에선 linear projection of flattened patches를 거쳐서 이미지를 임베딩한다.

최근엔 DALL-E 와 같이 CV와 NLP의 성격을 모두 지닌 모델들도 나오고 있다.

e.g) 아보카도 모양 의자를 만들라고하면 이미지를 만들어준다.

- 피어세션 & 질문 및 답변

# 2.깃헙특강_이고잉님_0812

## 0812 (목) 2번째 깃헙 특강

working directory → (add) → staging area → (commit) → repository(.git)

- Head(동그라미)는 현재 working directory가 어디인지를 가리킨다.
- 'checkout' 명령어를 사용하면 커밋해둔 이전 버전으로 갈 수 있다. 이렇게 시간여행을 가면 그곳에서 버그를 체크하고 수정할 수 있다. 이 놀라운 깃의 시간여행을 통해 우린 버그가 생겼을 때 전수조사가 아닌 버전을 체크해서 수정해준 후 merge해줄 수 있다.
- checkout(마스터 테두리 회색)  vs checkout branch (마스터 테두리 파랑)의 차이가 있는데, 이는 현재 켜져있는 branch가 어디냐를 나타내는 중요한 정보다.
- 시간여행할때는 그 버전으로 checkout하고,  시간여행 복귀할때는 master가 가리키는 그 branch로 checkout 한다.(checkout branch)
- 우리가 branch(가지)를 사용하는 이유는 1) 버리기가 쉽다(= 실험하기 쉽다) 2) 병합하기 쉽다(=협업하기 용이하다) 이 두가지이다.
- Head가 가리키는 branch 가 새로운 버전을 따라간다.
- exp 에 master merge == master에 exp merge , 결과는 똑같다.

    그러나 exp 에 master를 merge하는 것은 exp의 실험을 진행하기위해 필요한 정보들을 업데이트해주는 것이므로 자주해주는 것이 좋다. 그러나, master 에 exp를 merge해주는 것은 exp branch의 실험이 다 끝난 후에 해줘야 한다.

커밋할때 이유가없다면 바로 푸시해라!! 그게 나와 팀원들을 위해 도움이된다.

> 향후 필요할 때 볼 자료들

- 충돌나면 confilct를 그때 공부해라!!

[GIT3 - CLI Branch & 충돌](https://opentutorials.org/module/3927)

- pull request (merge request)

이걸 실행해놓고 개발하면 , 실시간으로 conflict(충돌)을 확인할 수 있다. 마찬가지로 필요할 때 공부하라.

[github.com - pull request](https://opentutorials.org/module/5083)

# 3.오피스아워

QnA

1. 딥러닝 시작할 때 어떤 논문 위주로 시작하면 좋을지 궁금합니다.
    1. NLP : RNN - LSTM - GRU - Transformer 이렇게 이어서 읽어야한다. (paper list 검색해서 순서대로 보면서 구현할 수 있으면 해보기!!)
    2. 제일 어려울 수학을 잘 봐야합니다 .. ^^
2. 아직 딥러닝에 대해 감이 안잡히는데 하나하나 이해하기 보다는 일단 해보는게 중요할까요?
    1. 하나하나 쉬운 것부터해야 어려운 것도 볼 수 있다. 다만, 해보지 않으면 이해하지 못하니 해보는 것 또한 중요하다. ⇒ 하나하나씩 해보면서 이해하기!!
    2. 모방은 제2의 창조다. 꼭 해봐라. 딥러닝은 가르치기도 어려운데 이유는 아는데 설명이 안되기 때문이다.. 그래서 해보면서 명확히 해가는 과정이 필요하다.
3. 멘토님들은 공부한 걸 정리할 때 어떤 매체에, 어느 기준으로(날짜별/항목별 등) 정리하셨는 지 궁금합니다.
    1. 예전에 노션몰라서 엑셀 썼는데 지금은 노션 추천!  
    2. 아이패드에 논문 저장해놓고 심심할 때 마다 읽었다. 다만 따로 정리해놓지 않아서 아쉽다.
    3. 강의안을 썼다. 그리고 심심할 때 논문 많이 읽었다. (정리는 못하고 있다)
4. 멘토님들은  이해가 안되는 문제를 만났을 때 바로 질문하는 편이신가요? 아니면 몇날이고 찾아보다가 질문하시는 편이신가요?
    1. 원래는 바로 질문했는데, 어느순간 깨닫고 몇날 공부하다가 찾아봤다.
    2. 빨리 배우는 만큼 빨리 잊어먹는다. 시간 투자하는 만큼 내 자산이 된다.  (수식은 2-3시간, 디버깅은 하루정도 쓴다.) 다만 주변에 물어보면서도 많이 배운다. 결론, 얼마나 고민해볼 지 정해두는 것도 좋다.
5. 회사에 갔다가 다시 학교로 돌아가는 것에 대해서 어떻게 생각하시는 지 궁금합니다. 가령 석사 후 취직하여 다시 박사를 하는 코스는 시간을 많이 낭비하는 것일까요?
    1. 현재에 주어진 것에 최대한 집중하는 걸 추천!!
    2. 시간을 많이 쓰는게 아니라 오히려 시간을 세이브하는 것이다. 회사 경험이 있으니 연구방향 설정, 프로젝트 경험에 능하다! 그리고 일을 하다보니 어떤 부분에서 연구를 해야할 지 잡혔다고 하는 경우가 있다!
6. 실제 현업에서는 어느정도의 모델을 사용하는 지 궁금합니다.
    1. 배포하는 거면 우리 지금 컴퓨터로 돌릴 수 있는 모델 사용한다. 
7. 아직 pytorch에 익숙하지 않아 예시를 보고 따라하는 식의 구현은 얼추 따라할 수 있지만, zero-base부터 구현하라고 하면 아직 고민이 된다. 공부법 추천 부탁!
    1. 케글에 있는 pytorch 이미지 분류 문제들 풀어보기! 
    2. 다음주가 파이토치니 커리큘럼만 잘따라가면 된다!
    - 급하면 채해요 여러분~ 아마 계속 트랜스포머 공부할 것이다. ! 이번주말은 트랜스포머 하나 뽀개자.
8. 최신동향
    1. CV : GAN , YOLO-X , 트랜스포머 
    2. NLP : GPT-3 , CLOVA LAMP 등 대규모 모델, AI 스피커(음성합성)
    3. 응용분야 (강화학습) : 지식추천(deep knowledge tracing) e.g)산타토익
