# 오늘의 회고 : 몰입하기 위해 실력 높이기 & 난이도 

1. 강의 내용
 

[Dataset ]
강의 목표 : 주어진 vanilla data를 Dataset으로 바꾸는 과정을 배울 것임

 

1. Pre-processing (전처리)

현업에선 제일 많은 시간이 소요되는 영역이다. (약 80%)
가끔 필요 이상으로 많은 정보를 가지고 있기도 한다 ⇒ Bounding box , 박스 밖에 부분은 노이즈로 본다.
계산의 효율을 위해 적당한 크기로 사이즈 변경 ⇒ Resize
"도메인, 데이터 형식에 따라 정말 다양한 Case가 존재" , 특히 의료데이터일 경우 전처리가 주요하게 작용한다. e.g) APTOS Blindness Detection (안구보고 질병 정도 예측)
하지만 전처리가 능사는 아니다. 좋아질 수도, 나빠질 수도 있다. 그렇기 떄문에 "실험을 통해 당위성을 부여"해야한다.
 

2. Generalization (일반화)

  1. Bias & Variance

(데이터를 충분히 학습하지 못해서) 학습이 너무 안 됐거나(High Bias - Underfitting)
(noise까지 학습할 정도로) 학습이 너무 잘 됐거나 (High Variance - Overfitting)
 

  2. Train & Validation 

학습 데이터 중 일부를 분리해서(모델이 보지 못한 데이터) , 검증 셋(학습이 잘 되는지 확인)으로 활용한다.

 


 

  3. Data Augmentation(증강)

주어진 데이터가 가질 수 있는 경우(Case) , 상태(State)의 다양성

How ? ⇒ torchvision.transforms (Random Crop , Flip)

Albumentations 라이브러리 : 더 빠르고 , 더 다양하다.

 

 

  4. "무조건" 이라는 단어를 제일 조심하세요!

항상 좋은 결과를 가져다 주지는 않습니다. 그저 하나의 도구일 뿐이다.
앞서 정의한 Problem(주제) 과 도메인을 깊이 이해하고, 질문하고, 실험으로 증명해야 한다.
 

 

[Data Generation ]
⇒ 데이터 셋을 잘 구성했다 해도, 잘 출력해야만 실속이 있다고 할 수 있다.

 

Data Feeding
Feed 란? 먹이를 주다.

즉, 대상의 상태를 고려해서 적정한 양을 준다.

Data Generator(10 batch/s) ⇒ Model(20 batch/s) = Max 10b/s
Data Generator(30 batch/s) ⇒ Model(20 batch/s) = Max 20b/s
⇒ Data Generator의 성능도 확인해봐야할 필요성이 있다.

예시#image_size = (300,300) trsfm = transforms.Compose([ transforms.ToTensor(), transforms.RandomRotation([-8,+8]), transforms.Resize(1024,1024)), ]) for i , data in enumerate(tqdm(dataset)): if i == 300: break
resize를 진행안할 경우 59 초
rotation 후 resize 할 경우 2분 53초
resize 후 rotation 할 경우 6분 16초
⇒ 순서, 적용내용이 바뀌면 성능의 차이가 발생함을 볼 수 있다.
torch.utils.data
Dataset의 구조
#torch.utils.data의 Dataset 라이브러리 상속 from torch.utils.data import Dataset #MyDataset 클래스가 처음 선언 되었을 때 호출 class MyDataset(Dataset): def __init__(self): print("Class init!") pass #MyDataset의 데이터 총 index 위치의 아이템을 리턴 def __getitem__(self,index): return index #MyDataset 아이템의 전체 길이 def __len__(self): print("length is 3") return 3

DataLoader
내가 만든 Dataset을 효율적으로 사용할 수 있도록 관련 기능 추가
My Dataset —————> DataLoader —————> (Batch, Channel, Height, Width)
엄청 기능이 많다.
여러번 실험을 해보면서 여러가지 옵션이 어떻게 상호작용하는 지 알아갈 수 있다.
collate_fn , 연구적으로 가면 자주 쓸 수 있다.
간단한 실험 : num_workders (스레드 갯수 지정)
엄연히 Dataset 과 DataLoader는 하는 일이 다르다.
Dataset : Vanilla data를 원하는 형태로 출력해주는 Class
DataLoader : Dataset을 더 효율적으로 사용하기 위함
즉 , DataLoader 와 Dataset class는 따로 만들고, DataLoader는 굳이 여러개 만들 필요가 없을 수 있다.
 

 

2. 공부하며 고민한 내용 & 고민한 결과
마스크 착용여부 & 성별에 대한 미스레이블들이 있다.
-> 미스레이블링된 것들은 제외해주고 학습해야겠다.어떻게?? 직접 빼주나??

데이터가 2700개 밖에 없어서 오버피팅될 확률이 높다.
-> 오버피팅을 피하기 위해 3가지 task를 따로해서 앙상블 해주는 건 어떨까?

나이 분류할 때 60대가 60살밖에 없다.
-> 60대 데이터만 증강해서 사용할 수 있으려나?

 

 

3. 몇개월 후에 봤을 때 도움될 자료
음... 몇개월 후엔 더 잘할거지? 화이팅

 

 

4. 회고 : 대회에 몰입하기 위해 두가지 해보기
어제 오늘을 돌아보면 P stage에 몰입하지 못하고 있는 나 자신을 발견했다.

이유를 돌아봤을 때 너무 한번에 어려운걸 하려다보니 그런건 아닐까 싶었다. (즉 내게 버겁다는 의미)

 

그래서 "함께 자라기" 책에서 읽은 두가지를 적용해보고자 한다.

 

첫째, 실력 높이기

나보다 잘하는 사람과 짝 프로그래밍 해보기 : 피어세션 때 얘기해보기
이전에 했던 비슷한 경험 참고하기 (AI RUSH) : 하루에 1%씩 나아진다는 마음으로 최선을 다하되 조급하지 말자.
도구의 도움 받기 : 일단 프로젝트 템플릿말고 쥬피터로 하자!
둘째, 난이도 낮추기

아기버전 목표하기 -> 매일 수업 이해하고 , 공식문서 따라가며 돌아가는 모델 만드는 것을 목표하기!
위 두가지 내용을 막상 적용하려니 내 상황과 잘 연결되지는 않았지만 그래도 조금씩 적용해보며 더 몰입해야지!

내일은 오늘보다 몰입해서 회고할 때 나 자신을 칭찬하자!

 

 

오늘의 피드백
+) 밤에 자기전에 피드백한 것

-) 대회에 몰입하지 않고 안주한 것
