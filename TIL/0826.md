# 오늘의 회고 : 많이 못해보는 것도 자산이다.

- 오류
    1. TypeError: forward() missing 1 required positional argument: 'x'

        상황: pretrained 모델 불러와서 사용하려니 오류남

        ```python
        import torchvision.models
        wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)
        ```

    2. KeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)' -albumentation 오류

### 1. 오늘 공부한 내용

1. 모델 디버깅

    ```python
    # 1. named_parameters() 를 이용하는 방식
    for param, weight in model.named_parameters():
        print(f"{param:20} - size: {weight.size()}")
        print(weight)
        print("-" * 100)
        print()
    ```

2. 인스턴스의 타입 확인

    ```python
    #아래와 같이 isinstance로 인스턴스 타입 확인가능
    print(f"model.state_dict() 의 Type : {type(model.state_dict())}")
    isinstance(model.state_dict(), OrderedDict)
    ```

3. 모델 저장하기 & 불러오기

    ```python
    import os

    #저장할 경로 생성 및 설정
    save_folder = './runs/'
    save_path = os.path.join(save_folder, "best.pth") 
    os.makedirs(save_folder, exist_ok=True)
    #모델 파라미터 저장 
    torch.save(model.state_dict(),save_path)
    print(f"{save_path} 폴더에 모델이 성공적으로 저장되었습니다.")
    print(f"해당 폴더의 파일 리스트: {os.listdir(save_folder)}")

    #불러오기
    new_model = MyModel()
    new_model.load_state_dict(torch.load(save_path))
    print(f"{save_path} 에서 성공적으로 모델을 load 하였습니다.")

    #잘 불러왔는 지 확인
    for (name,trained_weight), (_,saved_weight) in zip(model.named_parameters(), new_model.named_parameters()):
        is_equal = torch.equal(trained_weight,saved_weight)
        print(f"파라미터 {name:15} 에 대하여 trained 모델과 load 된 모델의 값이 같나요? -> {is_equal}")
    ```

    ⇒ .named parameters() 대신 .state_dict() 쓰는 이유는 param 뿐아니라 buffer도 같이 불러주기 떄문이다.

    병목현상 방지하기 (바로 GPU에 생성하는 게 효율적이다!)

    ```python
    #생성 후 이동
    torch.randn(2,2).cuda()

    #GPU에 바로 생성
    torch.randn(2,2,device=torch.device('cuda:0'))

    ```

     + 같은 device에 할당하지 않으면 두 텐서간 계산 불가능하다.!!

4. Torchvision 모델들은 보통 feature-extraction 파트, task-specific 파트로 크게 두 가지로 구성되어 있습니다. 사전학습모델 사용할 때 내 것에 맞게 task-specific 파트는 따로 바꿔줘야한다.

```python
num_classes = 18
model = vgg19_bn(pretrained=True)
model.classifier = nn.Sequential(
    nn.Linear(512 * 7 * 7, 4096),
    nn.ReLU(True),
    nn.Dropout(),
    nn.Linear(4096, 4096),
    nn.ReLU(True),
    nn.Dropout(),
    nn.Linear(4096, num_classes),
)

model
```

 + 모델에서 학습을 하고 싶지 않은 부분은 `.requires_grad_(False)` 로 freeze 시켜줄 수 있다.

1. timm에서 다양한 모델들 가져다 쓸 수 있다.

```python
!pip install timm

#모델 불러오기
import timm

m = timm.create_model('mobilenetv3_large_100', pretrained=True)
m.eval()

#모델 목록보기
import timm
from pprint import pprint

model_names = timm.list_models(pretrained=True)
pprint(model_names)

#모델 찾는 방법
import timm
from pprint import pprint

model_names = timm.list_models('*resne*t*')
pprint(model_names)
```

006359, 006360, 006361, 006362, 006363, 006364=> male4432, 1498-1=> female000020, 004418, 005227=> normal, incorrect

### 2. 오늘 생각해본 내용

1. 씨드 고정해서 변화해야할 부분만 바뀔 수 있도록 해야하는 걸로 아는데 정말 고정이 되는가?
2. 모델마다 적합한 옵티마이저를 뭘쓸 지 테스팅해봐야한다.
3. loss func (f1 loss 보다 라벨 스무딩 loss가 좋았다)
4. 단일모델이랑 앙상블 시도해봤을 때 별로 나아지지 않았다.
5. task 를 3개로 나눠서 , k-fold로 해봄
6. 60대 데이터 크롤링 해서 마스크 씌워보기.
7. 미스 레이블된 것들 찍어보기

 male : 006359, 006360, 006361, 006362, 006363, 006364

female : 4432, 1498-1 

normal, incorrect : 000020, 004418, 005227

### 3. 내일 봐야하는 내용 및 자료 (멘토링 포함)

1. kaggle 외부 데이터 : [https://www.kaggle.com/tapakah68/medical-masks-p4?select=images](https://www.kaggle.com/tapakah68/medical-masks-p4?select=images)
2. 모델 이어서 학습 하는 법 : [http://daplus.net/python-pytorch에서-훈련-된-모델을-저장하는-가장-좋은-방법은/](http://daplus.net/python-pytorch%EC%97%90%EC%84%9C-%ED%9B%88%EB%A0%A8-%EB%90%9C-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EC%A0%80%EC%9E%A5%ED%95%98%EB%8A%94-%EA%B0%80%EC%9E%A5-%EC%A2%8B%EC%9D%80-%EB%B0%A9%EB%B2%95%EC%9D%80/)
3. 학습 스케줄러 : [https://dacon.io/competitions/official/235697/codeshare/2373?page=1&dtype=recent](https://dacon.io/competitions/official/235697/codeshare/2373?page=1&dtype=recent)
- 멘토링 인사이트
    - Efficientnet b0에서부터 돌려보기.

    > 무거운거 가져와서 학습 이어서 해보기!! (조금씩 오른다) e.g) EfficientNet_b4

    - NfNet_f4같은 경우 정말 오래걸림
    - ViT patch16 (224, 224) 15 epoch - f1: 0.5 , NfNet_f4 (384,384) 7 epoch - f1: 0.55 , EfficientNet_b7 (600, 600) 10 epoch - f1: 0.6
    1. confusion matrix 찍어봐서 어떤 class를 모델이 못 맞추고 있는 지 확인해보기 + (오답 사진 출력도 가능하면?)
    2. 관련 논문 찾아보기 ( 나이 → Age classification )
    3. 모델의 featuremap 찍어서 모델이 어딜 보는 지 확인해보기
    4. seed만 바꿔가면서 학습해서 배깅해보기 

### 4. 오늘의 회고

지금은 조 내에서 제일 못하고 있다. 생각을 구현하는데 있어서 자꾸 에러가 나니까 기도 죽고, 기존에 선택과제들에서 조금씩 벌어진 격차가 내게 돌아오나 싶다. 하지만 이렇게 못하는 경험도 좋은 경험이고 자산이라고 생각한다.

이를 통해서 더욱 성장하고자하는 동기부여와 다른 사람들을 이해할 수 있는 포용심이 생기기를 기대한다.

그래도 오늘 잘한것은 우선 한바퀴 돌려서 제출해봤다. 그리고 이해가 되지 않는 부분은 기존 과제의 코드들을 하나하나 따라쳐보면서 이해하려고 했고 조금씩 이해가 됐다. 이제 금요일을 지나면 주말인데 계속해서 발전해가자!

> 오늘의 피드백

+) 제출해본 것, 계속해서 질문하고 상상하고 그것들을 정리한 것

-) 조급함에 내 코드를 짜기보다 다른 사람들의 코드를 더 보고싶어하는 마음? .. 

⇒ 조금만 더 인내함으로 성장의 시간을 보내자!! P stage는 앞으로도 많다!!!
